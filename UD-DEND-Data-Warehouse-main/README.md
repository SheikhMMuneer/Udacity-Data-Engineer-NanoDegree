# Udacity Data Engineering Nanodegree - Project 3/6  
[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg?style=flat-square&logo=Python)](https://www.python.org/)
[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg?style=flat-square&logo=Microsoft-Academic)](https://lbesson.mit-license.org/)


## **Data Warehouse**
---  
  

## Introduction
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.
As their data engineer, I am tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for the analytics team of Sparkify to continue finding insights in what songs their users are listening to.  


## Project Description
In this project, i will apply what i've learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. To complete the project, i will load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.  


## Requirements
This project was done on a macOS v11.1 with the source-code editor [Visual Studio Code](https://code.visualstudio.com/).

To implement the project you will need the following things:
- Python
- A cluster on AWS Redshift - more information you can find here [Getting started with Amazon Redshift](https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html)  

To use the module `psycopg2` with python you have to install it with the following command:

  ```bash
  pip install psycopg2-binary
  ```  


## Project Datasets
I'am working with two datasets that reside in S3. Here are the S3 links for each:
- Song data: `s3://udacity-dend/song_data`
- Log data: `s3://udacity-dend/log_data`

Log data json path: `s3://udacity-dend/log_json_path.json`  


### Song Dataset  
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
```json
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```  


### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.
![log-data](https://user-images.githubusercontent.com/32474126/102831228-886aac80-43eb-11eb-9601-cd7f4aa3eb79.png)  


## Database schema

### Staging tables
![ds_staging_tables](https://user-images.githubusercontent.com/32474126/102925999-8f99c500-4494-11eb-9053-4fb5a60b4491.png)

Loading from Json - Loss of numeric precision:  
In the staging table `staging_songs` I have chosen the data type TEXT instead of FLOAT for the columns artist_latitude, artist_longitude and duration.  
The [AWS Redshift documentation](https://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-copy-from-json.html) indicates that loading numbers from data files in JSON format may lose precision. These are then converted to FLOAT in the INSERT queries according to dim_artists.  

The data types of all other columns, including the staging_events columns, were specially selected based on their content.
I have not assigned keys such as SORTKEY or DISTKEY here, as these tables are only used for staging and no JOINS are executed on them.

### Fact and dimension tables
![ds_factdim_tables](https://user-images.githubusercontent.com/32474126/102926497-6463a580-4495-11eb-8eff-be67707df2d1.png)


I would like to briefly explain a few things about the assigned distribution keys and sort keys:  

fact_songplays:
- SORTKEY:
Here I choose the timestamp `start_time`, since the data analysts are likely to make more inquiries over a certain period of time. This makes queries more efficient because they can skip entire blocks that fall outside the time range.
- DISTKEY:
I choose a distribution key for the column `song_id` here, as most of the queries are carried out here.
I have not specified the distribution style here and therefore leave the selection to Redshift, as I trust AWS to make the right selection here.

dim_users:
- DISTKEY:
As mentioned above, the column song_id should receive a distribution key. This must also be noted in this table.

dim_time:
- SORTKEY:
Here, too, the column start_time should be sorted in order to speed up queries.


## How to run the scripts
You can start the scripts via the console. But first you have to jump to the correct folder path. Here is an example of running a python script:
```bash
python etl.py
```

## Explanation of the files in the project  

- `create_tables.py`
  - This function controls the dropping and creation of the tables
- `etl.py`
  - This function maps the ETL task in this project. The loading of the json files into the staging tables and the transfer of the data from the staging tables to the fact and dimension tables are triggered here.
- `sql_queries.py`
  - Contains all necessary queries for the above mentioned Python scripts. This script cannot be run on its own.
- `dwh_example.cfg`
  - Here I have provided an example of the necessary config file. This file contains a few explanations. If this file is filled with credentials, it should not be shared under any circumstances.

  
## Project summary
First I thought about and visualized the database schemas for the staging, fact and dimension tables.
Then I wrote the SQL statements to create and drop the tables. The addition of the `dwh.cfg` with the credentials and access data to the Redshift cluster was the next step. The testing of the create and drop statements could of course not be missing here.
Now it was time to complete the ETL pipeline in the Python script `etl.py`. This was also provided with exception handling and docstrings.
In the same way I have implemented exception handling and docstrings in `create_tables.py`.
As the penultimate step, the scripts `create_tables.py` and` etl.py` were checked for error-free function and the tables for correct content.
In the end I wrote this beautiful and hopefully clear README.
Have fun trying!
